{
  "cells": [
    {
      "metadata": {
        "id": "VwK5-9FIB-lu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": "# Sentiment/Customer Feedback Analysis By Using Python"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Importing the libraries"
    },
    {
      "metadata": {
        "id": "7QG7sxmoCIvN",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wTfaCIzdCLPA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": "## Importing the dataset"
    },
    {
      "metadata": {
        "id": "UCK6vQ5QCQJe",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)\ndataset.head()",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Review</th>\n      <th>Liked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Wow... Loved this place.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Crust is not good.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Not tasty and the texture was just nasty.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Stopped by during the late May bank holiday of...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The selection on the menu was great and so wer...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "                                              Review  Liked\n0                           Wow... Loved this place.      1\n1                                 Crust is not good.      0\n2          Not tasty and the texture was just nasty.      0\n3  Stopped by during the late May bank holiday of...      1\n4  The selection on the menu was great and so wer...      1"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Below comand shows there are total 1000 rows and 2 columns in the input dataset."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# shape Of the Data Set\nprint(dataset.shape)",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "(1000, 2)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "X1kiO9kACE6s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": "## Data Preprocessing"
    },
    {
      "metadata": {
        "id": "Qekztq71CixT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": "### Cleaning the texts\n#### Punctuations, Numbers don’t add any values to the final analysis. They will decrease the model efficiency. They can be removed from the input file.\n#### Roots of the word(Stemming) - Stemming is basically removing the suffix from a word and reduce it to its root word. As for example, by removing 'ing' from 'walking', we'll get the base word or root word which is “Walk”. This process will be applied to all the words in the input file.\n#### Stopwords are the most common words in any natural language. For the purpose of analyzing text data and building NLP models, these stopwords might not add much value,so, they need to be removed.  As for example, The, In, On, Over, Is and so on."
    },
    {
      "metadata": {
        "id": "8u_yXh9dCmEE",
        "colab_type": "code",
        "outputId": "bdcb9868-74c8-40b2-e5e9-877b949ce385",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "import re\nimport nltk\nnltk.download('stopwords')\n# To Remove Stop words\nfrom nltk.corpus import stopwords\n# To convert into Root Words\nfrom nltk.stem.porter import PorterStemmer\ncorpus = []\nfor i in range(0, 1000):\n  review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])\n  review = review.lower()\n  review = review.split()\n  ps = PorterStemmer()\n  all_stopwords = stopwords.words('english')\n  all_stopwords.remove('not')\n  review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n  review = ' '.join(review)\n  corpus.append(review)",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[nltk_data] Downloading package stopwords to /home/nbuser/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Changes after applying above code..."
    },
    {
      "metadata": {
        "id": "KpGWdrzGoAsL",
        "colab_type": "code",
        "outputId": "a1d5020d-8005-4735-d4b9-ad99fb366534",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "#print(corpus)\nprint(\"Before Applying The Code\")\nprint(\"------------------------\")\n#print(dataset.Review[:10].to_string(index=False))\nprint(list(dataset.Review[:10]))\nprint(\"After Applying The Code\")\nprint(\"------------------------\")\nprint(corpus[:10])    ",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Before Applying The Code\n------------------------\n['Wow... Loved this place.', 'Crust is not good.', 'Not tasty and the texture was just nasty.', 'Stopped by during the late May bank holiday off Rick Steve recommendation and loved it.', 'The selection on the menu was great and so were the prices.', 'Now I am getting angry and I want my damn pho.', \"Honeslty it didn't taste THAT fresh.)\", 'The potatoes were like rubber and you could tell they had been made up ahead of time being kept under a warmer.', 'The fries were great too.', 'A great touch.']\nAfter Applying The Code\n------------------------\n['wow love place', 'crust not good', 'not tasti textur nasti', 'stop late may bank holiday rick steve recommend love', 'select menu great price', 'get angri want damn pho', 'honeslti tast fresh', 'potato like rubber could tell made ahead time kept warmer', 'fri great', 'great touch']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CLqmAkANCp1-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": "## Creating the Bag of Words model using CountVectorizer"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### A bag-of-words is a representation of text that describes the occurrence of words. It involves two things:\n#### Vocabulary of known words.\n#### Measure of the presence of known words.\n\n#### From the cleaned dataset, potential features are extracted and are converted to numerical format. The vectorization techniques(Bag OF Words) are used to convert textual data to numerical format. Using this method, a matrix is created where each column represents a feature and each row represents an individual review."
    },
    {
      "metadata": {
        "id": "qroF7XcSCvY3",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.feature_extraction.text import CountVectorizer\n# \"max_features\" is used to get better results. To extract max 1500 feature. \ncv = CountVectorizer(max_features = 1500)\nX = cv.fit_transform(corpus).toarray()\ny = dataset.iloc[:, -1].values",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Saving these details for final submission.\n#X1 = cv.fit_transform(corpus).toarray()\nX1 = corpus\ny1 = dataset.iloc[:, -1].values\nfrom sklearn.model_selection import train_test_split\nX1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size = 0.20, random_state = 0)\n#X1_test\n#print(cv.inverse_transform(X1_test))",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DH_VjgPzC2cd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": "## Splitting the dataset into the Training set and Test set"
    },
    {
      "metadata": {
        "id": "qQXYM5VzDDDI",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VkIq23vEDIPt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": "## Training the Naive Bayes model on the Training set"
    },
    {
      "metadata": {
        "id": "DS9oiDXXDRdI",
        "colab_type": "code",
        "outputId": "77513c39-0ec6-4544-c056-26abe055b746",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n# print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint (\"Confusion Matrix:\\n\",cm)\n#accuracy_score(y_test, y_pred)\nacc_log1 = round(accuracy_score(y_test,y_pred)*100, 2)\nprint('accuracy percentage on test dataset is', acc_log1)",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Confusion Matrix:\n [[55 42]\n [12 91]]\naccuracy percentage on test dataset is 73.0\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Training the Random Forest model on the Training set"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.ensemble import RandomForestClassifier \nclassifier = RandomForestClassifier(n_estimators = 501, \n                            criterion = 'entropy') \n                              \nclassifier.fit(X_train, y_train) \n# Predicting the Test set results\ny_pred = classifier.predict(X_test) \n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint (\"Confusion Matrix:\\n\",cm)\n#accuracy_score(y_test, y_pred)\nacc_log2 = round(accuracy_score(y_test,y_pred)*100, 2)\nprint('accuracy percentage on test dataset is', acc_log2)",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_420/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n  from numpy.core.umath_tests import inner1d\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "Confusion Matrix:\n [[89  8]\n [39 64]]\naccuracy percentage on test dataset is 76.5\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "### Training the Logistic Regression model on the Training set"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn import linear_model\nclassifier = linear_model.LogisticRegression(C=1.5)\nclassifier.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint (\"Confusion Matrix:\\n\",cm)\n#accuracy_score(y_test, y_pred)\nacc_log3 = round(accuracy_score(y_test,y_pred)*100, 2)\nprint('accuracy percentage on test dataset is', acc_log3)\n",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Confusion Matrix:\n [[79 18]\n [27 76]]\naccuracy percentage on test dataset is 77.5\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Evaluating The Best Model"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "models = pd.DataFrame({\n    'Model': ['Naive Bayes', 'Random Forest', 'Logistic Regression'],\n    'Score': [acc_log1, acc_log2, acc_log3]})\nmodels.sort_values(by='Score', ascending=False)",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>Logistic Regression</td>\n      <td>77.5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Random Forest</td>\n      <td>76.5</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Naive Bayes</td>\n      <td>73.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "                 Model  Score\n2  Logistic Regression   77.5\n1        Random Forest   76.5\n0          Naive Bayes   73.0"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Predecting The Test Data By Using The Logistic Regression Algorithm And Writing The Test Data + Predicted Column Into CSV File"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df1 = pd.DataFrame(X1_test)\ndf2 = pd.DataFrame(y_pred)\ndf2.columns = ['Predicted Type']\nsubmission = pd.concat([df1,df2],axis = 1)\nsubmission.to_csv('submission.csv', index=False)\nsubmission",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>Predicted Type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>present food aw</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>worst food servic</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>never dine place</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>guess mayb went night disgrac</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>sushi lover avoid place mean</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>ambianc much better</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>hole wall great mexican street taco friendli s...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>food bad enough enjoy deal world worst annoy d...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>never ever go back</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>atmospher fun</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>pancak also realli good pretti larg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>tapa dish delici</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>chain fan beat place easili</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>everyon attent provid excel custom servic</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>staff also friendli effici</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>love friendli server great food wonder imagin ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>disappoint dinner went elsewher dessert</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>perhap caught night judg review not inspir go ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>disgust pretti sure human hair</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>sweet potato fri good season well</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>like final blow</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>place jewel la vega exactli hope find nearli t...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>fantast food</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>order toast english muffin came untoast</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>order albondiga soup warm tast like tomato sou...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>staff super nice quick even crazi crowd downto...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>thing like prime rib dessert section</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>love mussel cook wine reduct duck tender potat...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>food good servic good price good</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>crust not good</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>170</th>\n      <td>chipolt ranch dip saus tasteless seem thin wat...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>171</th>\n      <td>good go review place twice herea tribut place ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>172</th>\n      <td>bathroom clean place well decor</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>173</th>\n      <td>food qualiti horribl</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>174</th>\n      <td>wors humili worker right front bunch horribl n...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>175</th>\n      <td>wonton thin not thick chewi almost melt mouth</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>176</th>\n      <td>horribl wast time money</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>177</th>\n      <td>great food</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>178</th>\n      <td>servic extrem slow</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>179</th>\n      <td>nice blanket moz top feel like done cover subp...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>180</th>\n      <td>block amaz</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>181</th>\n      <td>wast enough life pour salt wound draw time too...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>182</th>\n      <td>due fact took minut acknowledg anoth minut get...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>183</th>\n      <td>bu boy hand rude</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>184</th>\n      <td>would avoid place stay mirag</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>185</th>\n      <td>everi time eat see care teamwork profession degre</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>186</th>\n      <td>tonight elk filet special suck</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>187</th>\n      <td>best tater tot southwest</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>188</th>\n      <td>bother slow servic</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>189</th>\n      <td>servic friendli invit</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>190</th>\n      <td>excel</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>191</th>\n      <td>one tabl thought food averag worth wait</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>192</th>\n      <td>huge awkward lb piec cow th gristl fat</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>193</th>\n      <td>get angri want damn pho</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>194</th>\n      <td>hot sour egg flower soup absolut star</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>195</th>\n      <td>servic slow not attent</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>196</th>\n      <td>food great alway compliment chef</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>197</th>\n      <td>waitress good though</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>198</th>\n      <td>not much flavor poorli construct</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>199</th>\n      <td>fianc came middl day greet seat right away</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>200 rows × 2 columns</p>\n</div>",
            "text/plain": "                                                     0  Predicted Type\n0                                      present food aw               0\n1                                    worst food servic               0\n2                                     never dine place               0\n3                        guess mayb went night disgrac               0\n4                         sushi lover avoid place mean               0\n5                                  ambianc much better               0\n6    hole wall great mexican street taco friendli s...               1\n7    food bad enough enjoy deal world worst annoy d...               0\n8                                   never ever go back               0\n9                                        atmospher fun               1\n10                 pancak also realli good pretti larg               1\n11                                    tapa dish delici               1\n12                         chain fan beat place easili               1\n13           everyon attent provid excel custom servic               1\n14                          staff also friendli effici               1\n15   love friendli server great food wonder imagin ...               1\n16             disappoint dinner went elsewher dessert               0\n17   perhap caught night judg review not inspir go ...               0\n18                      disgust pretti sure human hair               0\n19                   sweet potato fri good season well               1\n20                                     like final blow               0\n21   place jewel la vega exactli hope find nearli t...               0\n22                                        fantast food               1\n23             order toast english muffin came untoast               1\n24   order albondiga soup warm tast like tomato sou...               1\n25   staff super nice quick even crazi crowd downto...               1\n26                thing like prime rib dessert section               1\n27   love mussel cook wine reduct duck tender potat...               1\n28                    food good servic good price good               1\n29                                      crust not good               0\n..                                                 ...             ...\n170  chipolt ranch dip saus tasteless seem thin wat...               0\n171  good go review place twice herea tribut place ...               1\n172                    bathroom clean place well decor               1\n173                               food qualiti horribl               0\n174  wors humili worker right front bunch horribl n...               0\n175      wonton thin not thick chewi almost melt mouth               0\n176                            horribl wast time money               0\n177                                         great food               1\n178                                 servic extrem slow               0\n179  nice blanket moz top feel like done cover subp...               1\n180                                         block amaz               1\n181  wast enough life pour salt wound draw time too...               1\n182  due fact took minut acknowledg anoth minut get...               0\n183                                   bu boy hand rude               0\n184                       would avoid place stay mirag               0\n185  everi time eat see care teamwork profession degre               0\n186                     tonight elk filet special suck               0\n187                           best tater tot southwest               1\n188                                 bother slow servic               0\n189                              servic friendli invit               1\n190                                              excel               1\n191            one tabl thought food averag worth wait               0\n192             huge awkward lb piec cow th gristl fat               0\n193                            get angri want damn pho               0\n194              hot sour egg flower soup absolut star               1\n195                             servic slow not attent               0\n196                   food great alway compliment chef               1\n197                               waitress good though               1\n198                   not much flavor poorli construct               0\n199         fianc came middl day greet seat right away               1\n\n[200 rows x 2 columns]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "natural_language_processing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}